{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 Topic Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Aabharan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Aabharan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ§  Topic #1:\n",
            "models model learning data using dataset segmentation performance training datasets\n",
            "\n",
            "ðŸ§  Topic #2:\n",
            "generative learning data model image models neural propose methods framework\n",
            "\n",
            "ðŸ§  Topic #3:\n",
            "network learning spatial model training data models energy computational time\n",
            "\n",
            "ðŸ§  Topic #4:\n",
            "methods metrics information performance different framework propose domain propagation tasks\n",
            "\n",
            "ðŸ§  Topic #5:\n",
            "learning models network modeling model approach networks analysis layers challenges\n"
          ]
        }
      ],
      "source": [
        "# 03_topic_clustering.ipynb\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"../data/raw/arxiv_papers.csv\")\n",
        "\n",
        "# Preprocess summaries\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return \" \".join([word for word in tokens if word.isalpha() and word not in stop_words])\n",
        "\n",
        "df[\"processed_summary\"] = df[\"summary\"].apply(preprocess)\n",
        "\n",
        "# Convert text to bag-of-words format\n",
        "vectorizer = CountVectorizer(max_df=0.9, min_df=5, stop_words='english')\n",
        "doc_term_matrix = vectorizer.fit_transform(df[\"processed_summary\"])\n",
        "\n",
        "# Apply LDA using sklearn\n",
        "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "# Show topics\n",
        "words = vectorizer.get_feature_names_out()\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    print(f\"\\nðŸ§  Topic #{idx + 1}:\")\n",
        "    print(\" \".join([words[i] for i in topic.argsort()[-10:][::-1]]))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
